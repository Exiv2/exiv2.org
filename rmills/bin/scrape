#!/usr/bin/env python3
import sys
from   bs4 import BeautifulSoup as Soup
import ssl
import argparse
import io
import gzip
import urllib.request

def scrape(argv):
	options=['cols2','photos','img']
	# parse command-line
	if len(argv) != 3:
		print('Number of arguments:', len(sys.argv), 'arguments.')
		print('Argument List:', str(sys.argv))
		print('usage: ' + sys.argv[0] + ' url','option')
		print('options:',options)
		sys.exit(1)
	url	   = argv[1]
	option = argv[2]
	
	# print('url=',url,'option=',option)

	# Read the page from the url
	ctxt = ssl._create_unverified_context()
	# page = urllib.request.urlopen(url, context=ctxt)
	
	req = urllib.request.Request(url)
	req.add_header('Accept', '*/*')
	req.add_header('User-Agent','scrape')
	req.add_header('Accept-encoding', 'gzip')
	response = urllib.request.urlopen(req,context=ctxt)
	if response.getheader("Content-Encoding") == "gzip":
		page = gzip.decompress(response.read())
	else:
		page = response.read().decode('utf-8')
	
	soup = Soup(page,features='lxml')
	
	try:
		i=options.index(option)
		if i == options.index('cols2'):
			# find and report every image
			for div in soup.findAll('div', attrs={'id':'cols2'}):
				for i in div.contents:
					if i != '\n':
						print(i,)
		elif i == options.index('photos'):
			images = soup.findAll('img')
			for image in images:
				url=image['src']
				if len(url) > 120:	# ignore short urls, they are icons
					url = url.split('=')[0]+'=w1920-h1080';
					print('<object data="'+url+'"></object>')
		elif i == options.index('img'):
			images = soup.findAll('img')
			for image in images:
				print(image['src'])
	except:
		print('unknown option',option)
		print('valid options',options)

##
#
if __name__ == '__main__':
	scrape(sys.argv)
			
